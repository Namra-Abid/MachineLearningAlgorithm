{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91d212b-4035-4b56-bf91-2c6237b385ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06b179-fa89-4c58-a3df-b37e3298eda3",
   "metadata": {},
   "source": [
    " the standard Newton's method formula for finding the root of a function \\( f(x) = 0 \\) is:\r\n",
    "\r\n",
    "$$\r\n",
    "x_{n} = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}\r\n",
    "$$\r\n",
    "\r\n",
    "This formula is used to iteratively update the value of \\( x \\) until \\( f(x) \\) is close to zero. \r\n",
    "\r\n",
    "However, in the context of optimization, Newton's method is used to find the minimum (or maximum) of a function, not just its root. In this case, the update formula for the parameter vector \\( \\theta \\) is modified accordingly to move towards the minimum of the loss function, hence the formula provided earlier:\r\n",
    "\r\n",
    "$$\r\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\left( \\nabla^2 \\text{Loss}(\\theta_{\\text{old}}) \\right)^{-1} \\nabla \\text{Loss}(\\theta_{\\text{old}})\r\n",
    "$$\r\n",
    "\r\n",
    "Here, \\( \\nabla \\text{Loss}(\\theta) \\) represents the gradient of the loss function, and \\( \\nabla^2 \\text{Loss}(\\theta) \\) represents the Hessian matrix of the loss function. This modification allows Newton's method to iteratively update the parameters towards the minimum of the loss function in the parameter space.\r\n",
    "ameter space.\r\n",
    "ameter space.\r\n",
    "ameter space.\r\n",
    "ameter space.\r\n",
    "rameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5103fdb0-303d-4177-bb21-10e90ca6fb87",
   "metadata": {},
   "source": [
    "In the context of logistic regression and Newton's method, let's break down the formulas for calculating the gradient and Hessian matrix:\n",
    "\n",
    "Gradient of the loss function with respect to the parameters (\\( \\theta \\)):\n",
    "\n",
    "$$\n",
    "\\nabla Loss(\\theta) = \\frac{1}{N} X^{T} \\cdot (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( X \\) is the design matrix (including the bias term).\n",
    "- \\( \\hat{y} \\) is the predicted probability vector.\n",
    "- \\( y \\) is the true label vector.\n",
    "- \\( N \\) is the number of samples.\n",
    "\n",
    "Hessian matrix of the loss function with respect to the parameters (\\( \\theta \\)):\n",
    "\n",
    "$$\n",
    "\\nabla^{2} Loss(\\theta) = \\frac{1}{N} X^{T} \\cdot D \\cdot X\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( D \\) is a diagonal matrix with elements \\( \\hat{y}_i \\cdot (1 - \\hat{y}_i) \\) for each sample \\( i \\).\n",
    "\n",
    "These formulas represent the first and second derivatives of the loss function with respect to the parameters, which are used in Newton's method for optimization.\n",
    "method for optimization.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4663824b-95a5-4790-aba9-d6d00aa69724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy.special import expit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Helper Functions\n",
    "class Functions:\n",
    "    ## function for random sampling of data\n",
    "    def random_sampler(self, perc=0.1):\n",
    "        prop = int(len(self.X) * perc) #proportion of the data to be used for each iteration, here we have set it to 10% of the data\n",
    "        ind = np.random.choice(range(len(self.X)), prop, replace=False)\n",
    "        test_ind = list(set(range(len(self.X))) - set(ind)) #for using the rest sample as test set\n",
    "        return self.X[ind], self.y[ind], self.X[test_ind], self.y[test_ind]\n",
    "    \n",
    "    ## function for normalizing the features\n",
    "    def normalize_features(self, X, append=True):\n",
    "        X = (X - np.mean(X, 0)) / np.std(X, 0) #normalize the features\n",
    "        if append:\n",
    "            X = np.append(np.ones(X.shape[0]).reshape(-1,1), X, 1) #append column of ones for intercept\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f74e92c0-aaa1-4f4f-a866-38b496328c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y.values.reshape(-1,1)\n",
    "        \n",
    "    def fit(self, max_iter=100, tolerance=1e-6, lr=0.001):         \n",
    "        # Initialize Beta with small random values\n",
    "        np.random.seed(0)\n",
    "        B = np.random.randn(self.X.shape[1], 1) * 0.01\n",
    "        \n",
    "        # function for updating parameters using newton's method\n",
    "        loss_list = []\n",
    "        test_loss_list = []\n",
    "        x, y, x_test, y_test = self.random_sampler(perc=0.80)  # use 80-20 in the training too\n",
    "        \n",
    "        for it in range(max_iter):\n",
    "            # calculate training loss\n",
    "            curr_loss, gradient, hessian = self.cross_entropy_loss(x, y, B)\n",
    "            loss_list.append(curr_loss)\n",
    "            \n",
    "            # calculate test loss\n",
    "            curr_loss_test, _, _ = self.cross_entropy_loss(x_test, y_test, B)\n",
    "            test_loss_list.append(curr_loss_test)\n",
    "            \n",
    "            # newton method\n",
    "            e = -np.linalg.solve(hessian, gradient)\n",
    "            B = B + lr * e\n",
    "            \n",
    "            # stop loop once updation becomes insignificant\n",
    "            if np.all(np.abs(e) < tolerance):\n",
    "                break\n",
    "                \n",
    "        self.B = B\n",
    "        return B, loss_list, test_loss_list\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = np.matmul(X, self.B)\n",
    "        # Avoiding division by zero\n",
    "        exp_scores = np.exp(-scores)\n",
    "        return np.where(exp_scores < 1e-10, 1.0, 1. / (1. + exp_scores))\n",
    "\n",
    "    def cross_entropy_loss(self, X, y, B):\n",
    "        N = len(X)\n",
    "        p = 1. / (1. + np.exp(-np.dot(X, B)))\n",
    "        W = np.diag((p * (1 - p)).reshape(-1))\n",
    "        return (-1 / N * (np.dot(y.T, np.log(p)) + np.dot(np.transpose(1 - y), np.log(1 - p))))[0][0], -np.dot(X.T, (y - p)), X.T @ W @ X\n",
    "\n",
    "    def random_sampler(self, perc=0.8):\n",
    "        # Randomly shuffle the data\n",
    "        indices = np.random.permutation(self.X.shape[0])\n",
    "        split_index = int(self.X.shape[0] * perc)\n",
    "        train_indices, test_indices = indices[:split_index], indices[split_index:]\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        x_train, x_test = self.X[train_indices], self.X[test_indices]\n",
    "        y_train, y_test = self.y[train_indices], self.y[test_indices]\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "720929b0-c777-431a-995d-2c48efbf50f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>X24</th>\n",
       "      <th>X25</th>\n",
       "      <th>X26</th>\n",
       "      <th>X27</th>\n",
       "      <th>X28</th>\n",
       "      <th>X29</th>\n",
       "      <th>X30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y     X1     X2     X3      X4       X5       X6      X7       X8      X9  \\\n",
       "0  M  17.99  10.38  122.8  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  M  20.57  17.77  132.9  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  M  19.69  21.25  130.0  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "\n",
       "   ...    X21    X22    X23     X24     X25     X26     X27     X28     X29  \\\n",
       "0  ...  25.38  17.33  184.6  2019.0  0.1622  0.6656  0.7119  0.2654  0.4601   \n",
       "1  ...  24.99  23.41  158.8  1956.0  0.1238  0.1866  0.2416  0.1860  0.2750   \n",
       "2  ...  23.57  25.53  152.5  1709.0  0.1444  0.4245  0.4504  0.2430  0.3613   \n",
       "\n",
       "       X30  \n",
       "0  0.11890  \n",
       "1  0.08902  \n",
       "2  0.08758  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading the data\n",
    "log_reg_data = pd.read_csv('logistic.csv')\n",
    "log_reg_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93f08336-299f-418e-83aa-79b552edb39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>X24</th>\n",
       "      <th>X25</th>\n",
       "      <th>X26</th>\n",
       "      <th>X27</th>\n",
       "      <th>X28</th>\n",
       "      <th>X29</th>\n",
       "      <th>X30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.371429</td>\n",
       "      <td>14.117635</td>\n",
       "      <td>19.185033</td>\n",
       "      <td>91.882242</td>\n",
       "      <td>654.377582</td>\n",
       "      <td>0.095744</td>\n",
       "      <td>0.103619</td>\n",
       "      <td>0.088898</td>\n",
       "      <td>0.048280</td>\n",
       "      <td>0.181099</td>\n",
       "      <td>...</td>\n",
       "      <td>16.235103</td>\n",
       "      <td>25.535692</td>\n",
       "      <td>107.103121</td>\n",
       "      <td>876.987033</td>\n",
       "      <td>0.131532</td>\n",
       "      <td>0.252742</td>\n",
       "      <td>0.274595</td>\n",
       "      <td>0.114182</td>\n",
       "      <td>0.290502</td>\n",
       "      <td>0.083868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.483719</td>\n",
       "      <td>3.535815</td>\n",
       "      <td>4.266005</td>\n",
       "      <td>24.322027</td>\n",
       "      <td>354.943187</td>\n",
       "      <td>0.013923</td>\n",
       "      <td>0.052470</td>\n",
       "      <td>0.079468</td>\n",
       "      <td>0.038060</td>\n",
       "      <td>0.027487</td>\n",
       "      <td>...</td>\n",
       "      <td>4.811267</td>\n",
       "      <td>6.065108</td>\n",
       "      <td>33.374664</td>\n",
       "      <td>567.672841</td>\n",
       "      <td>0.023083</td>\n",
       "      <td>0.155014</td>\n",
       "      <td>0.209398</td>\n",
       "      <td>0.065326</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>0.017848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.691000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>47.920000</td>\n",
       "      <td>170.400000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>...</td>\n",
       "      <td>8.678000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>54.490000</td>\n",
       "      <td>223.600000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.705000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.100000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.085825</td>\n",
       "      <td>0.062890</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>0.161850</td>\n",
       "      <td>...</td>\n",
       "      <td>13.055000</td>\n",
       "      <td>21.045000</td>\n",
       "      <td>84.255000</td>\n",
       "      <td>516.450000</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.144950</td>\n",
       "      <td>0.116550</td>\n",
       "      <td>0.063930</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.071050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>18.680000</td>\n",
       "      <td>85.980000</td>\n",
       "      <td>551.700000</td>\n",
       "      <td>0.094620</td>\n",
       "      <td>0.090970</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033410</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.220000</td>\n",
       "      <td>97.670000</td>\n",
       "      <td>686.600000</td>\n",
       "      <td>0.130900</td>\n",
       "      <td>0.210100</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.098610</td>\n",
       "      <td>0.282700</td>\n",
       "      <td>0.080060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.740000</td>\n",
       "      <td>21.585000</td>\n",
       "      <td>103.750000</td>\n",
       "      <td>767.600000</td>\n",
       "      <td>0.104550</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.132350</td>\n",
       "      <td>0.073895</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>...</td>\n",
       "      <td>18.410000</td>\n",
       "      <td>29.690000</td>\n",
       "      <td>124.650000</td>\n",
       "      <td>1031.500000</td>\n",
       "      <td>0.146050</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.317750</td>\n",
       "      <td>0.092070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.218400</td>\n",
       "      <td>0.937900</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Y          X1          X2          X3           X4  \\\n",
       "count  455.000000  455.000000  455.000000  455.000000   455.000000   \n",
       "mean     0.371429   14.117635   19.185033   91.882242   654.377582   \n",
       "std      0.483719    3.535815    4.266005   24.322027   354.943187   \n",
       "min      0.000000    7.691000    9.710000   47.920000   170.400000   \n",
       "25%      0.000000   11.705000   16.170000   75.100000   420.300000   \n",
       "50%      0.000000   13.300000   18.680000   85.980000   551.700000   \n",
       "75%      1.000000   15.740000   21.585000  103.750000   767.600000   \n",
       "max      1.000000   28.110000   39.280000  188.500000  2501.000000   \n",
       "\n",
       "               X5          X6          X7          X8          X9  ...  \\\n",
       "count  455.000000  455.000000  455.000000  455.000000  455.000000  ...   \n",
       "mean     0.095744    0.103619    0.088898    0.048280    0.181099  ...   \n",
       "std      0.013923    0.052470    0.079468    0.038060    0.027487  ...   \n",
       "min      0.052630    0.019380    0.000000    0.000000    0.116700  ...   \n",
       "25%      0.085825    0.062890    0.029320    0.020340    0.161850  ...   \n",
       "50%      0.094620    0.090970    0.061540    0.033410    0.179200  ...   \n",
       "75%      0.104550    0.131300    0.132350    0.073895    0.195800  ...   \n",
       "max      0.163400    0.311400    0.426800    0.201200    0.304000  ...   \n",
       "\n",
       "              X21         X22         X23          X24         X25  \\\n",
       "count  455.000000  455.000000  455.000000   455.000000  455.000000   \n",
       "mean    16.235103   25.535692  107.103121   876.987033    0.131532   \n",
       "std      4.811267    6.065108   33.374664   567.672841    0.023083   \n",
       "min      8.678000   12.020000   54.490000   223.600000    0.071170   \n",
       "25%     13.055000   21.045000   84.255000   516.450000    0.114400   \n",
       "50%     14.970000   25.220000   97.670000   686.600000    0.130900   \n",
       "75%     18.410000   29.690000  124.650000  1031.500000    0.146050   \n",
       "max     36.040000   49.540000  251.200000  4254.000000    0.218400   \n",
       "\n",
       "              X26         X27         X28         X29         X30  \n",
       "count  455.000000  455.000000  455.000000  455.000000  455.000000  \n",
       "mean     0.252742    0.274595    0.114182    0.290502    0.083868  \n",
       "std      0.155014    0.209398    0.065326    0.063151    0.017848  \n",
       "min      0.027290    0.000000    0.000000    0.156500    0.055040  \n",
       "25%      0.144950    0.116550    0.063930    0.249500    0.071050  \n",
       "50%      0.210100    0.226400    0.098610    0.282700    0.080060  \n",
       "75%      0.341600    0.387200    0.161100    0.317750    0.092070  \n",
       "max      0.937900    1.252000    0.291000    0.663800    0.173000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's replace the categorical variable with M being 1 and B being 0\n",
    "log_reg_data['Y'] = log_reg_data['Y'].replace({'M':1,'B':0})\n",
    "train_df, test_df = train_test_split(log_reg_data, test_size=0.2, random_state=42)\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d7875d-1df5-4fb6-a10c-ac417cf0a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing outliers using IQR\n",
    "Q1 = train_df.quantile(0.25,numeric_only=True)\n",
    "Q3 = train_df.quantile(0.75,numeric_only=True)\n",
    "IQR = Q3 - Q1 # difference between the third and first quartiles\n",
    "train_df = train_df[~((train_df < (Q1 - 1.5 * IQR)) | (train_df > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "824ac9be-7489-49ab-98a2-bd04a35928a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splititng data into x and y\n",
    "X = log_reg_data.iloc[:,1:]\n",
    "y = log_reg_data.iloc[:,0]\n",
    "#let's split the data into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#Normalizing the data\n",
    "X_train_normalized = Functions().normalize_features(X_train)\n",
    "X_test_normalized = Functions().normalize_features(X_test)\n",
    "# fitting the model\n",
    "log_reg_model = LogisticRegression(X_train_normalized,y_train)\n",
    "log_reg_model_params,train_loss,test_loss = log_reg_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ac10c36-00c9-4384-a3cb-ea58d80e1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_preds = log_reg_model.predict(X_test_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c4d6f58-bd3f-4075-9089-6145d45bc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import expit\n",
    "\n",
    "# Helper Functions\n",
    "class Functions:\n",
    "    ## function for random sampling of data\n",
    "    def random_sampler(self, X, y, perc=0.1):\n",
    "        prop = int(len(X) * perc) #proportion of the data to be used for each iteration, here we have set it to 10% of the data\n",
    "        ind = np.random.choice(range(len(X)), prop, replace=False)\n",
    "        test_ind = list(set(range(len(X))) - set(ind)) #for using the rest sample as test set\n",
    "        return X[ind], y[ind], X[test_ind], y[test_ind]\n",
    "    \n",
    "    ## function for normalizing the features\n",
    "    def normalize_features(self, X, append=True):\n",
    "        X = (X - np.mean(X, 0)) / np.std(X, 0) #normalize the features\n",
    "        if append:\n",
    "            X = np.append(np.ones(X.shape[0]).reshape(-1,1), X, 1) #append column of ones for intercept\n",
    "        return X\n",
    "\n",
    "class Logistic_Regression():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y.values.reshape(-1,1)\n",
    "        \n",
    "    def fit(self, max_iter=100, tolerance=1e-6, lr=0.001):         \n",
    "        # Initialize Beta with small random values\n",
    "        np.random.seed(0)\n",
    "        B = np.random.randn(self.X.shape[1], 1) * 0.01\n",
    "        \n",
    "        # function for updating parameters using newton's method\n",
    "        x, y, _, _ = Functions().random_sampler(self.X, self.y, perc=0.80)  # use 80-20 in the training too\n",
    "        \n",
    "        for it in range(max_iter):\n",
    "            # calculate training loss\n",
    "            _, gradient, hessian = self.cross_entropy_loss(x, y, B)\n",
    "            \n",
    "            # newton method\n",
    "            e = -np.linalg.solve(hessian, gradient)\n",
    "            B = B + lr * e\n",
    "            \n",
    "            # stop loop once updation becomes insignificant\n",
    "            if np.all(np.abs(e) < tolerance):\n",
    "                break\n",
    "                \n",
    "        self.B = B\n",
    "        return B\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = np.matmul(X, self.B)\n",
    "        # Avoiding division by zero\n",
    "        exp_scores = np.exp(-scores)\n",
    "        return np.where(exp_scores < 1e-10, 1.0, 1. / (1. + exp_scores))\n",
    "\n",
    "    def cross_entropy_loss(self, X, y, B):\n",
    "        N = len(X)\n",
    "        p = 1. / (1. + np.exp(-np.dot(X, B)))\n",
    "        W = np.diag((p * (1 - p)).reshape(-1))\n",
    "        loss = (-1 / N * (np.dot(y.T, np.log(p)) + np.dot(np.transpose(1 - y), np.log(1 - p))))[0][0]\n",
    "        gradient = -np.dot(X.T, (y - p))\n",
    "        hessian = X.T @ W @ X\n",
    "        return loss, gradient, hessian\n",
    "\n",
    "## reading the data\n",
    "log_reg_data = pd.read_csv('logistic.csv')\n",
    "# Let's replace the categorical variable with M being 1 and B being 0\n",
    "log_reg_data['Y'] = log_reg_data['Y'].replace({'M':1,'B':0})\n",
    "\n",
    "# Removing outliers using IQR\n",
    "Q1 = log_reg_data.quantile(0.25, numeric_only=True)\n",
    "Q3 = log_reg_data.quantile(0.75, numeric_only=True)\n",
    "IQR = Q3 - Q1 # difference between the third and first quartiles\n",
    "log_reg_data = log_reg_data[~((log_reg_data < (Q1 - 1.5 * IQR)) | (log_reg_data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Splititng data into x and y\n",
    "X = log_reg_data.iloc[:, 1:]\n",
    "y = log_reg_data.iloc[:, 0]\n",
    "\n",
    "# Let's split the data into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizing the data\n",
    "X_train_normalized = Functions().normalize_features(X_train)\n",
    "X_test_normalized = Functions().normalize_features(X_test)\n",
    "\n",
    "# Fitting the model\n",
    "log_reg_model1 = Logistic_Regression(X_train_normalized, y_train)\n",
    "log_reg_model_params1 = log_reg_model1.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a0e649b-8e8f-4ef1-b097-3cea80ee92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training loss for log_reg_model1\n",
    "train_loss_list1 = []\n",
    "for x, y in zip(X_train_normalized, y_train):\n",
    "    y = np.array([y]).reshape(1, 1)  # Convert y to numpy array and reshape\n",
    "    p = log_reg_model1.predict(x.reshape(1, -1))  # Calculate probability using predict method\n",
    "    curr_loss, _, _ = log_reg_model1.cross_entropy_loss(x.reshape(1, -1), y, log_reg_model_params1)  # Ensure x is an array\n",
    "    train_loss_list1.append(curr_loss)\n",
    "\n",
    "# Calculate test loss for log_reg_model1\n",
    "test_loss_list1 = []\n",
    "for x, y in zip(X_test_normalized, y_test):\n",
    "    y = np.array([y]).reshape(1, 1)  # Convert y to numpy array and reshape\n",
    "    p = log_reg_model1.predict(x.reshape(1, -1))  # Calculate probability using predict method\n",
    "    curr_loss, _, _ = log_reg_model1.cross_entropy_loss(x.reshape(1, -1), y, log_reg_model_params1)  # Ensure x is an array\n",
    "    test_loss_list1.append(curr_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "967a6c8d-7c3c-4d8a-8a13-f154dd0d4e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4899550861106548,\n",
       " 0.6468907863195028,\n",
       " 0.5986691135564758,\n",
       " 0.6815639765758792,\n",
       " 0.5569019377024386,\n",
       " 0.5742134596576609,\n",
       " 0.6119750555269736,\n",
       " 0.6771432304378617,\n",
       " 0.5818859212246589,\n",
       " 0.6286559743914196,\n",
       " 0.5883014230944422,\n",
       " 0.5585517634305328,\n",
       " 0.5984429047818006,\n",
       " 0.5801195168534707,\n",
       " 0.5397610397494413,\n",
       " 0.567105582704365,\n",
       " 0.5212148605340438,\n",
       " 0.5243548852163936,\n",
       " 0.651996079714507,\n",
       " 0.658784732636071,\n",
       " 0.5927552502728928,\n",
       " 0.5113208098476771,\n",
       " 0.6719046771890299,\n",
       " 0.5910890106299694,\n",
       " 0.4828913726802667,\n",
       " 0.648064045907902,\n",
       " 0.4669693852369213,\n",
       " 0.6240618498009951,\n",
       " 0.6142157411906369,\n",
       " 0.6359129794113787,\n",
       " 0.5374896163881792,\n",
       " 0.635288779546336,\n",
       " 0.7179497559299064,\n",
       " 0.6040093045642512,\n",
       " 0.6317061541509656,\n",
       " 0.4848296857888362,\n",
       " 0.5886360352215703,\n",
       " 0.5231943577706903,\n",
       " 0.6434411022723633,\n",
       " 0.6039800026592133,\n",
       " 0.6699359755207712,\n",
       " 0.5647347523615027,\n",
       " 0.6088093361896832,\n",
       " 0.6151764257329493,\n",
       " 0.588874963918671,\n",
       " 0.6640287812160139,\n",
       " 0.6321520566791573,\n",
       " 0.6582870846901662,\n",
       " 0.7009164136975109,\n",
       " 0.5751570719930551,\n",
       " 0.6401207594669871,\n",
       " 0.5331364632688542,\n",
       " 0.573678494329382,\n",
       " 0.580741924282084,\n",
       " 0.5758826721382976,\n",
       " 0.6035438732819819,\n",
       " 0.6358359017443357,\n",
       " 0.7152751106772239,\n",
       " 0.5968019617996694,\n",
       " 0.57851517423284,\n",
       " 0.5764394835467674,\n",
       " 0.5919398946516232,\n",
       " 0.598193760805261,\n",
       " 0.6202608557782634,\n",
       " 0.5688959207860034,\n",
       " 0.5080812684401939,\n",
       " 0.6063702666837896,\n",
       " 0.6132163775792048,\n",
       " 0.5695644136771338,\n",
       " 0.5303372060065131,\n",
       " 0.4859223537189939,\n",
       " 0.6307100182209402,\n",
       " 0.6285594192489093,\n",
       " 0.4157561438926684,\n",
       " 0.4948255715834833,\n",
       " 0.5828051557255162,\n",
       " 0.5057691901895537,\n",
       " 0.5774619844704476,\n",
       " 0.5594018987017204,\n",
       " 0.5465197002176907,\n",
       " 0.5506564121960258,\n",
       " 0.5344246490154674,\n",
       " 0.6356407619866108,\n",
       " 0.6777595473481054,\n",
       " 0.5742309877990229,\n",
       " 0.5452808511110484,\n",
       " 0.609877513968756,\n",
       " 0.6628483346120579,\n",
       " 0.5861473648862414,\n",
       " 0.6602899894287737,\n",
       " 0.6036950881541381,\n",
       " 0.6486077228683967,\n",
       " 0.6200021578399163,\n",
       " 0.6266459758026098,\n",
       " 0.6222190670786242,\n",
       " 0.7058761723391728,\n",
       " 0.5641682899985553,\n",
       " 0.536065669073227,\n",
       " 0.6790402119204386,\n",
       " 0.5706469853535029,\n",
       " 0.7168412619392711,\n",
       " 0.5997104393348643,\n",
       " 0.5410394959316723,\n",
       " 0.45320077885785875,\n",
       " 0.5893274696927892,\n",
       " 0.6593578440699953,\n",
       " 0.6197738302462932,\n",
       " 0.5460567561252063,\n",
       " 0.5590650456168811,\n",
       " 0.6388530482584884,\n",
       " 0.598918658876509,\n",
       " 0.5511297935613563,\n",
       " 0.6006815214315674,\n",
       " 0.5744000171141338,\n",
       " 0.5946387223858788,\n",
       " 0.5973693966212739,\n",
       " 0.580483602366792,\n",
       " 0.6138473462385243,\n",
       " 0.4523283422801707,\n",
       " 0.5598818791130827,\n",
       " 0.6296104268055587,\n",
       " 0.587869575325927,\n",
       " 0.6580961025067424,\n",
       " 0.6696767848008596,\n",
       " 0.6709270652651907,\n",
       " 0.587176728237247,\n",
       " 0.6570672764915592,\n",
       " 0.6138246533400599,\n",
       " 0.5895270594033128,\n",
       " 0.5872220523751323,\n",
       " 0.6226624458740798,\n",
       " 0.6123520385117731,\n",
       " 0.620582869230533,\n",
       " 0.6089606677033274,\n",
       " 0.6266354044249475,\n",
       " 0.5671271065173052,\n",
       " 0.7020946903966493,\n",
       " 0.6101311579630447,\n",
       " 0.5990018712144546,\n",
       " 0.5845855330665817,\n",
       " 0.6638368808670478,\n",
       " 0.693207429857414,\n",
       " 0.6955454031127751,\n",
       " 0.601584862762168,\n",
       " 0.6674593978399029,\n",
       " 0.5673210711455738,\n",
       " 0.623679135044914,\n",
       " 0.5507731942781496,\n",
       " 0.5455985122695025,\n",
       " 0.5973892712034471,\n",
       " 0.6397379981805954,\n",
       " 0.5967005963199975,\n",
       " 0.5694921068429998,\n",
       " 0.6441041500370122,\n",
       " 0.6057641011632227,\n",
       " 0.5475165070168746,\n",
       " 0.47040339056241626,\n",
       " 0.6750581145901764,\n",
       " 0.4284708930663976,\n",
       " 0.6675216300099858,\n",
       " 0.6070067924088147,\n",
       " 0.6171320687064115,\n",
       " 0.5747841966956753,\n",
       " 0.5070493381785384,\n",
       " 0.6737361041150147,\n",
       " 0.6235329550400142,\n",
       " 0.5966901110371541,\n",
       " 0.6338957357998961,\n",
       " 0.699145136553528,\n",
       " 0.5976484446424528,\n",
       " 0.5785738656906148,\n",
       " 0.583014827122593,\n",
       " 0.610229904601123,\n",
       " 0.4883876000444314,\n",
       " 0.5779442460802098,\n",
       " 0.6104023892814079,\n",
       " 0.5752137216362327,\n",
       " 0.6024121159852805,\n",
       " 0.5834665965472059,\n",
       " 0.5902855425615056,\n",
       " 0.45169470951347085,\n",
       " 0.5551915034319979,\n",
       " 0.6202600827058983,\n",
       " 0.6117750636975501,\n",
       " 0.5542395669894246,\n",
       " 0.6218310457563412,\n",
       " 0.5698770537908202,\n",
       " 0.5599003070020105,\n",
       " 0.6476056134645692,\n",
       " 0.6281191315369551,\n",
       " 0.6212412282291072,\n",
       " 0.5464562970814546,\n",
       " 0.6306389468079552,\n",
       " 0.6324302377730793,\n",
       " 0.6020318480089223,\n",
       " 0.7005437201436724,\n",
       " 0.5768078393062214,\n",
       " 0.6468863905088407,\n",
       " 0.5401345468243387,\n",
       " 0.6277122450498388,\n",
       " 0.5971824872038075,\n",
       " 0.660599493926648,\n",
       " 0.6072178977316394,\n",
       " 0.5854522046119579,\n",
       " 0.62164876720701,\n",
       " 0.5376484254808697,\n",
       " 0.6032317055799569,\n",
       " 0.5648623708694753,\n",
       " 0.5640067659849988,\n",
       " 0.6236057367886659,\n",
       " 0.48041148843099823,\n",
       " 0.5244976452773119,\n",
       " 0.5969003784269831,\n",
       " 0.5332610307717144,\n",
       " 0.597543629591448,\n",
       " 0.6275695876127034,\n",
       " 0.6611680048020618,\n",
       " 0.5783102130516682,\n",
       " 0.5841713242327177,\n",
       " 0.560545506269366,\n",
       " 0.5782435319670214,\n",
       " 0.6200802147261032,\n",
       " 0.5594743365421355,\n",
       " 0.5645738779475419,\n",
       " 0.5819506356337635,\n",
       " 0.5683456631775727,\n",
       " 0.6028651531695582,\n",
       " 0.6000639033870893,\n",
       " 0.4915710730402971,\n",
       " 0.5613523216559194,\n",
       " 0.5530648927205036,\n",
       " 0.5266222738943291,\n",
       " 0.4778316321856789,\n",
       " 0.5949440096721932,\n",
       " 0.4635098068336535,\n",
       " 0.605618804368749,\n",
       " 0.5473683530527751,\n",
       " 0.5586035301069625,\n",
       " 0.5830482978504534,\n",
       " 0.5593129827603021,\n",
       " 0.5860713417182534,\n",
       " 0.6634782619611539,\n",
       " 0.615569884392594,\n",
       " 0.5380196346901346,\n",
       " 0.6012726097424106,\n",
       " 0.623044550138059,\n",
       " 0.6412928072839017,\n",
       " 0.5620447857059029,\n",
       " 0.6095753701843573,\n",
       " 0.6413807584236042,\n",
       " 0.5745751163722129,\n",
       " 0.6969261837137743,\n",
       " 0.5878155077634575,\n",
       " 0.5809423405579069,\n",
       " 0.624620827662673,\n",
       " 0.6250007254060593,\n",
       " 0.6131816001339736,\n",
       " 0.6897605706562572,\n",
       " 0.5618759786106657,\n",
       " 0.5684857909613246,\n",
       " 0.6007832059228746,\n",
       " 0.5809478873091766,\n",
       " 0.5370177949040534,\n",
       " 0.6000551958493416,\n",
       " 0.690053571701691,\n",
       " 0.6191535247750904,\n",
       " 0.6117650780490791,\n",
       " 0.6700056107559171,\n",
       " 0.5123604951929296,\n",
       " 0.5462546550267173,\n",
       " 0.5832323037392791,\n",
       " 0.4379112531772682,\n",
       " 0.5911406149265056,\n",
       " 0.5806589157614716,\n",
       " 0.6338376761564348,\n",
       " 0.5957108850851761,\n",
       " 0.7288079192686711,\n",
       " 0.643638208391036,\n",
       " 0.5485917244006231,\n",
       " 0.5612208583361269,\n",
       " 0.6867572235103329,\n",
       " 0.5608496315492683,\n",
       " 0.5969687700069849,\n",
       " 0.6412427010678219,\n",
       " 0.6016285104071917,\n",
       " 0.5364960778942545,\n",
       " 0.6578577616471825,\n",
       " 0.6417050441872817,\n",
       " 0.6388160403154878,\n",
       " 0.5589602046252926,\n",
       " 0.6158907513843869,\n",
       " 0.6247262075581987,\n",
       " 0.6530936625043472,\n",
       " 0.5895339060327847,\n",
       " 0.5962574438330154,\n",
       " 0.5975521637560015,\n",
       " 0.6777944396268184,\n",
       " 0.6074880444595439,\n",
       " 0.6122727547340778,\n",
       " 0.5376211387625511,\n",
       " 0.5942747141400112,\n",
       " 0.6709888748087625,\n",
       " 0.545806668968932,\n",
       " 0.6065252485827983,\n",
       " 0.5358228186118906,\n",
       " 0.6264430685150933,\n",
       " 0.5059240991607239,\n",
       " 0.5527432127832091,\n",
       " 0.5720303975882108,\n",
       " 0.5772266790666152,\n",
       " 0.5734460728174204,\n",
       " 0.5278397792063989,\n",
       " 0.5540294043315861,\n",
       " 0.566048240226371,\n",
       " 0.5838815911184627,\n",
       " 0.6062993883644587,\n",
       " 0.6629087134021618,\n",
       " 0.6052020959440424]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2543134-2ce2-4b1c-9741-7c5dfc60fbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a96de327-5b58-41d3-b776-a8dc512455ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Ensure y_train is extracted as a numpy array\n",
    "# y_train = y_train.values\n",
    "\n",
    "# # Calculate training loss\n",
    "# train_loss_list = []\n",
    "# for x, y in zip(X_train_normalized, y_train):\n",
    "#     y = np.array([y])  # Convert y to numpy array\n",
    "#     p = log_reg_model.predict(x.reshape(1, -1))  # Calculate probability using predict method\n",
    "#     curr_loss, _, _ = log_reg_model.cross_entropy_loss(x, y, log_reg_model_params)\n",
    "#     train_loss_list.append(curr_loss)\n",
    "\n",
    "# # Calculate test loss\n",
    "# test_loss_list = []\n",
    "# for x, y in zip(X_test_normalized, y_test):\n",
    "#     y = np.array([y])  # Convert y to numpy array\n",
    "#     p = log_reg_model.predict(x.reshape(1, -1))  # Calculate probability using predict method\n",
    "#     curr_loss, _, _ = log_reg_model.cross_entropy_loss(x, y, log_reg_model_params)\n",
    "#     test_loss_list.append(curr_loss)\n",
    "\n",
    "# # Optionally, you can plot or print these losses if needed\n",
    "# print(\"Train Loss:\", train_loss_list)\n",
    "# print(\"Test Loss:\", test_loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66254c03-4091-4685-8f1c-1d68f6441fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c2b6c-8faa-4ada-b1aa-8eeeef0edb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d066a9f-7b3e-49b9-9c72-1c048a164641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
